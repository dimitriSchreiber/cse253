{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./Data/input.txt\")\n",
    "text = file.readlines()\n",
    "file.close()\n",
    "\n",
    "text_array = np.asarray(text)\n",
    "\n",
    "#Creates list of each song\n",
    "indicies = np.where(text_array == '<start>\\n') #Location of where each abc file starts\n",
    "data = []\n",
    "for i in range(len(indicies[0])):\n",
    "    if i+1 == len(indicies[0]):\n",
    "        #For the last abc file\n",
    "        abc = text_array[indicies[0][i]:]\n",
    "    else:\n",
    "        abc = text_array[indicies[0][i]:indicies[0][i+1]]\n",
    "    data.append(''.join(abc))\n",
    "\n",
    "#print(data[0])\n",
    "#print(data[1123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899\n",
      "225\n",
      "[   0    1    2 ... 1121 1122 1123]\n",
      "[ 752  893 1050 ...  835  559  684]\n",
      "899\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "#80 - 20 split on data -> training and validation\n",
    "#Constants\n",
    "train_len = int(np.floor(len(data)*0.8))\n",
    "validation_len = len(data) - train_len\n",
    "\n",
    "print(train_len)\n",
    "print(validation_len)\n",
    "\n",
    "np.random.seed(0)\n",
    "#Each index references to a single song\n",
    "indxs = np.asarray(range(len(data)))\n",
    "print(indxs)\n",
    "np.random.shuffle(indxs)\n",
    "print(indxs)\n",
    "\n",
    "temp = np.asarray(data)\n",
    "train_data = temp[indxs[0:train_len]]\n",
    "validation_data = temp[indxs[train_len:]]\n",
    "print(len(train_data))\n",
    "print(len(validation_data))\n",
    "\n",
    "#print(temp[1050])\n",
    "#print(train_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ascii_vals = []\n",
    "validation_ascii_vals = []\n",
    "\n",
    "for songs in train_data:\n",
    "    for chars in songs:\n",
    "        train_ascii_vals.append(ord(chars))\n",
    "        \n",
    "for songs in validation_data:\n",
    "    for chars in songs:\n",
    "        validation_ascii_vals.append(ord(chars))\n",
    "        \n",
    "#Each character is a row in this notation\n",
    "onehot_train = np.eye(128)[train_ascii_vals]\n",
    "train_labels = train_ascii_vals\n",
    "onehot_validation = np.eye(128)[validation_ascii_vals]\n",
    "validation_labels = validation_ascii_vals\n",
    "# print(train_ascii_vals[0])\n",
    "# print(onehot_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM constants\n",
    "input_size = 128 #num_classes = 128\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "batch_first = True\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run first in single batches, each batch has 25 chars per(sequence length 25), just run it in order of the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_rnn(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers=1,batch_size=1,\n",
    "                 sequence_length=25,batch_first = True):\n",
    "        super(lstm_rnn, self).__init__()\n",
    "        #Constants\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        #Initializing model\n",
    "        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first = batch_first)\n",
    "        \n",
    "    def forward(self,data,initial):\n",
    "        #data = data.view(self.batch_size, self.sequence_length, self.input_size)\n",
    "        output,hidden = self.lstm(data,initial)\n",
    "        return output,hidden\n",
    "        \n",
    "    def init_hidden(self,zero=1):\n",
    "        if zero == 0:\n",
    "            #Random initialization\n",
    "            initial_hidden = autograd.Variable(torch.randn(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        else:\n",
    "            #Zero initialization\n",
    "            initial_hidden = autograd.Variable(torch.zeros(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        return initial_hidden\n",
    "    \n",
    "    def init_cell(self,zero=1):\n",
    "        if zero == 0:\n",
    "            #Random initialization\n",
    "            initial_cell = autograd.Variable(torch.randn(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        else:\n",
    "            #Zero initialization\n",
    "            initial_cell = autograd.Variable(torch.zeros(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        return initial_cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n",
      "1\n",
      "25\n",
      "1\n",
      "lstm_rnn(\n",
      "  (lstm): LSTM(128, 128, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm = lstm_rnn(input_size,hidden_size,num_layers,batch_size,\n",
    "                sequence_length,batch_first = batch_first)\n",
    "print(lstm.input_size)\n",
    "print(lstm.hidden_size)\n",
    "print(lstm.batch_size)\n",
    "print(lstm.sequence_length)\n",
    "print(lstm.num_layers)\n",
    "print(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=0.1)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16061\n"
     ]
    }
   ],
   "source": [
    "#Creates batch indicies, +1 since arange is [,), exclusive last element\n",
    "batch_indicies = np.arange(0,len(onehot_train)-sequence_length+1,sequence_length)\n",
    "print(len(batch_indicies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "h_0 = lstm.init_hidden()\n",
    "c_0 = lstm.init_cell()\n",
    "states = (h_0,c_0)\n",
    "loss = []\n",
    "accuracy = []\n",
    "\n",
    "#CAN ADD LINEAR LAYER AND BATCH PROCESSING \n",
    "for epoch in range(5):\n",
    "    for indx in batch_indicies:\n",
    "        optimizer.zero_grad()\n",
    "        temp_data = onehot_train[indx:indx+25]\n",
    "        temp_data = torch.from_numpy(temp_data).float()\n",
    "        temp_data = autograd.Variable(temp_data.view(batch_size,sequence_length,input_size))\n",
    "    \n",
    "        #output is of whole sequence, states hidden is same as last element of output, also\n",
    "        #has cell hidden state\n",
    "        output, states = lstm(temp_data,states)\n",
    "        \n",
    "        h_0 = autograd.Variable(states[0].data, requires_grad=True)\n",
    "        c_0 = autograd.Variable(states[1].data, requires_grad=True)\n",
    "        states = (h_0,c_0)\n",
    "        #hidden = Variable(hidden.data, requires_grad=True)\n",
    "\n",
    "        \n",
    "        #Add 1 since we are using the next letter as the label\n",
    "        labels = autograd.Variable(torch.LongTensor(train_labels[indx+1:indx+25+1]))\n",
    "        \n",
    "        #Later try adding a linear layer here\n",
    "        \n",
    "        loss_temp = criterion(output.view(sequence_length,input_size),labels)\n",
    "        loss.append(loss_temp.data)\n",
    "        loss_temp.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        vals,max_indxs = (output.data).max(2)\n",
    "        acc_temp = (max_indxs==(labels.data)).sum()\n",
    "        accuracy.append(acc_temp/sequence_length)\n",
    "        #print(loss)\n",
    "        #print(accuracy)\n",
    "    print(epoch)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function for this later \n",
    "primer = \"<start>\\n\"\n",
    "start_token = []\n",
    "for chars in primer:\n",
    "    start_token.append(ord(chars))\n",
    "    \n",
    "start_token = np.eye(128)[start_token]\n",
    "start_token = torch.from_numpy(start_token).float()\n",
    "start_token = autograd.Variable(start_token.view(1,8,input_size))\n",
    "#print(start_token.argmax(1))\n",
    "\n",
    "#write a function for this later \n",
    "end = \"<end>\"\n",
    "end_token = []\n",
    "for chars in end:\n",
    "    end_token.append(ord(chars))\n",
    "    \n",
    "end_token = np.eye(128)[end_token]\n",
    "\n",
    "done = 1\n",
    "\n",
    "#Generating music\n",
    "h_0 = lstm.init_hidden()\n",
    "c_0 = lstm.init_cell()\n",
    "states = (h_0,c_0)\n",
    "chars = []\n",
    "\n",
    "#Each loop generates 1 char\n",
    "for i in range(100):\n",
    "    output,states = lstm(start_token,states)\n",
    "    \n",
    "    h_0 = autograd.Variable(states[0].data, requires_grad=True)\n",
    "    c_0 = autograd.Variable(states[1].data, requires_grad=True)\n",
    "    states = (h_0,c_0)\n",
    "    \n",
    "    #Currently I am taking the max of the hidden state as the letter that it\n",
    "    #produces, the TA on piazza said that it is not ideal to take max,\n",
    "    #can try implementing what he says later\n",
    "    val,indx = states[0].data.max(2)\n",
    "    chars.append(indx)\n",
    "    \n",
    "    start_token = torch.from_numpy(np.eye(128)[indx]).float()\n",
    "    start_token = autograd.Variable(start_token.view(1,1,input_size))\n",
    "    \n",
    "    #Need to implement a check for \"<end>\", generation should stop once\n",
    "    #\"<end>\" is produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting back to strings\n",
    "outputs = []\n",
    "for letter in chars:\n",
    "    outputs.append(chr(letter))\n",
    "\n",
    "print(chars)\n",
    "print(outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
